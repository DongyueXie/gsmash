---
title: "Normal mean penalized form optimization - Compound method"
author: "Dongyue Xie"
date: "2022-05-25"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction

\[y|\mu\sim N(\mu,s^2),\mu\sim g(\cdot)\]

Optimization: $\theta = E_q\mu$,

\[\min_{\theta,g}h(\theta,g) = \frac{1}{2s^2}(y-\theta)^2+\rho_{g,s}(\theta).\]


\[S_{g,s^2}(z) = z +s^2l'_{NM}(z;g,s^2)\]


The penalty term is only tractable at $\rho(S(\cdot))$. There are two ways to accommodate this. 

Define the penalty evaluated at $S_{g,s^2}(\cdot)$ as $c_{g,s^2}(\cdot):=\rho_{g,s^2}(S_{g,s^2}(\cdot))$.

1. Compound way


Let $\theta = S_{g,s^2}(z)$ such that $\rho_{g,s^2}(\theta) = c_{g,s^2}(z)$, the optimization problem is 


\[\min_{z,g} \tilde h(z,g) = \frac{1}{2s^2}(y-S_{g,s^2}(z))^2-l_{NM}(z;g,s^2)-(z-S_{g,s^2}(z))^2/(2s^2)\]

\[\min_{z,g} \tilde h(z,g) = \frac{1}{2s^2}(y-z-s^2l'_{NM}(z;g,s^2))^2-l_{NM}-s^2(l'_{NM})^2/2\]


Then set $\hat\theta = S(z)$. 

2. Inversion way


Note that $c_{g,s^2}(S^{-1}_{g,s^2}(\theta)) = \rho_{g,s^2}(\theta)$, so we can write the optimization problem as 

\[\min_{\theta,g}  h(\theta,g) = \frac{1}{2s^2}(y-\theta)^2-c_{g,s^2}(S^{-1}_{g,s^2}(\theta))\]

For the inversion method implementation and illustration, see [here](normal_mean_penalized_optimization_inversion.html)



```{r}
# #'log marginal likelihood of normal mean model
# #'@param x scalar 
# #'@param g prior, normal mixture
# l_nm = function(x,s,w,grid){
#   log(sum(w*dnorm(x,0,sqrt(grid^2+s^2))))
# }
# 
# #'first order derivative of log marginal likelihood of normal mean model
# #'@param x scalar 
# #'@param g prior, normal mixture
# l_nm_d1 = function(x,s,w,grid){
#   f = sum(w*dnorm(x,0,sqrt(grid^2+s^2)))
#   f_d1 = sum(w*dnorm(x,0,sqrt(grid^2+s^2))*x/(grid^2+s^2))
#   f_d1/f
# }
# 
# softmax = function(a){
#   exp(a-max(a))/sum(exp(a-max(a)))
# }
# 
# #'posterior mean operator
# S = function(x,s,w,grid){
#   K = length(w)
#   g = normalmix(pi=w,mean=rep(0,K),sd=grid)
#   fit.ash = ash(x,s,g=g,fixg=T)
#   fit.ash$result$PosteriorMean
# }

source("code/poisson_mean/pois_mean_penalized.R")
```

generate data, and get grid by running ash

```{r,warning=FALSE}
set.seed(12345)
n = 200
w0 = 0.9
lambda = c(rep(0,round(n*w0)),rep(10,n-round(n*w0)))
w_true = c(w0,1-w0)
grid_true = c(0.01,7)
s = 1
y = rnorm(n,lambda,s)
library(ashr)
fit.ash = ashr::ash(y,s,mixcompdist = 'normal')
#grid = exp(seq(log(s/100),log(sqrt(max(abs(y^2-s^2)))),by=log(sqrt(2))))
#fit.ash = S(y,s,w_true,grid_true)
#plot(fit.ash$fitted_g$sd,fit.ash$fitted_g$pi)
grid = fit.ash$fitted_g$sd
grid = grid[-1]
K = length(grid)

#plot(y,main='ash fit',col='grey80')
#lines(lambda,col='grey60')
#lines(fit.ash$result$PosteriorMean)
#legend('topleft',c('data','true mean','ash posteriorMean'),pch=c(1,NA,NA),lty=c(NA,1,1),col=c('grey80','grey60',1))
```

## Known prior

We start with known prior, when $g$ is fully specified.

When the prior is known, we are able to perform optimization one by one.

```{r}
#'objective function with known g
#'@param theta mu_bar
#'@param y,s a single observation and its std
#'@param w
#'@param grid prior sds
f_obj_known_g = function(theta,y,s,w,grid){
  
  res = (y-theta-s^2*l_nm_d1_z(theta,s,w,grid))^2/2/s^2 - l_nm(theta,s,w,grid) - s^2*(l_nm_d1_z(theta,s,w,grid))^2/2
  
  return(res)
}

ebnm_penalized_compound_known_g = function(x,s,w,grid,z_init = NULL,opt_method = 'L-BFGS-B'){
  n = length(x)
  z = double(n)
  if(length(s)==1){
    s = rep(s,n)
  }
  if(is.null(z_init)){
    z_init = x
  }
  for(i in 1:n){
    z[i] = optim(z_init[i],f_obj_known_g,
                   y=x[i],
                   s=s[i],
                   w=w,
                   grid=grid,
                   method = opt_method)$par
  }
  posteriorMean = S(z,s,w,grid)
  return(list(z=z,posteriorMean=posteriorMean))
}
```

```{r}
ploter = function(fit,y,lambda,main='known prior'){
  plot(y,main=main,col='grey80')
  lines(lambda,col='grey60')
  lines(fit$z,type='p',pch=20,col='grey80')
  lines(fit$posteriorMean)
  legend('topleft',c('data','z','true mean','estimated mean'),pch=c(1,20,NA,NA),lty=c(NA,NA,1,1),col=c('grey80','grey80','grey60',1))
}
```

### Init $z$ at data $y$.

```{r}
fit = ebnm_penalized_compound_known_g(y,s,w_true,grid_true,z_init = y,opt_method = 'L-BFGS-B')
```

```{r}
ploter(fit,y,lambda,main='known prior, init z at y')
```


### Init $z$ at 0.

```{r}
fit = ebnm_penalized_compound_known_g(y,s,w_true,grid_true,z_init = double(n),opt_method = 'L-BFGS-B')
```

```{r}
ploter(fit,y,lambda,main='known prior, init z at 0')
```

## Estimate prior

```{r}
#'objective function
#'@param theta (mu_bar,w)
#'@param grid prior sds
f_obj = function(theta,y,s,grid){
  n = length(y)
  w = softmax(theta[-(1:n)])
  z = theta[1:n]
  
  res = sum((y-z-s^2*l_nm_d1_z(z,s,w,grid))^2/2/s^2 - l_nm(z,s,w,grid) - s^2*(l_nm_d1_z(z,s,w,grid))^2/2)

  return(res)
}

ebnm_penalized_compound = function(x,s,grid,z_init = NULL,w_init=NULL,opt_method = 'L-BFGS-B'){
  n = length(x)
  K = length(grid)
  if(is.null(w_init)){
    w_init = rep(1/K,K)
  }
  if(length(s)==1){
    s = rep(s,n)
  }
  if(is.null(z_init)){
    z_init = x
  }
  out = optim(c(z_init,w_init),f_obj,method=opt_method,
              y=y,grid=grid,s=s)
  z = out$par[1:n]
  w = softmax(out$par[-(1:n)])
  posteriorMean = S(z,s,w,grid)
  return(list(z=z,w=w,posteriorMean=posteriorMean,opt_res = out))
}

```

### init at y

```{r}
fit = ebnm_penalized_compound(y,s,grid,z_init = y)
plot(grid,fit$w,ylab='w hat')
fit$opt_res$value
```


```{r}
ploter(fit,y,lambda,main='estimate prior, init z at y')
```

### init at 0

```{r}
fit = ebnm_penalized_compound(y,s,grid,z_init = rep(0,n))
plot(grid,fit$w,ylab='w hat')
fit$opt_res$value
```

```{r}
ploter(fit,y,lambda,main='estimate prior, init z at 0')
```

### ash fit

```{r,warning=FALSE}
fit.ash = ash(y,s,mixcompdist = 'normal',pointmass=FALSE,prior='uniform',mixsd=grid)
plot(y,main='ash fit',col='grey80')
lines(lambda,col='grey60')
lines(fit.ash$result$PosteriorMean)
legend('topleft',c('data','true mean','ash posteriorMean'),pch=c(1,NA,NA),lty=c(NA,1,1),col=c('grey80','grey60',1))
```




